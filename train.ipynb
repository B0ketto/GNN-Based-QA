{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c12a041-5fed-4b5f-bb4d-7d4c4283ec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.data import DataLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f75340e-1393-4100-abc7-de26fb310a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# === GNN Ranker Model ===\n",
    "class GNNRanker(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GNNRanker, self).__init__()\n",
    "        \n",
    "        #Mean Aggregation\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Max Aggrgation\n",
    "        # self.conv1 = SAGEConv(input_dim, hidden_dim, aggr='max')\n",
    "        # self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr='max')\n",
    "\n",
    "        # Min Aggrgation\n",
    "        # self.conv1 = SAGEConv(input_dim, hidden_dim, aggr='min')\n",
    "        # self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr='min')\n",
    "\n",
    "        # Sum Aggrgation\n",
    "        # self.conv1 = SAGEConv(input_dim, hidden_dim, aggr='sum')\n",
    "        # self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr='sum')\n",
    "\n",
    "        self.scorer = nn.Linear(hidden_dim, 1)\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return self.scorer(x).squeeze(-1)  # shape: [num_nodes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b2cea21-f953-41c2-b548-429008acd870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Loss Function ===\n",
    "def ranking_loss(scores, positive_ids):\n",
    "    loss = 0.0\n",
    "    pos_scores = scores[positive_ids]\n",
    "    all_indices = torch.arange(len(scores), device=scores.device)\n",
    "    neg_indices = list(set(all_indices.tolist()) - set(positive_ids.tolist()))\n",
    "    \n",
    "    if len(neg_indices) == 0 or len(pos_scores) == 0:\n",
    "        return torch.tensor(0.0, device=scores.device, requires_grad=True)\n",
    "\n",
    "    neg_scores = scores[neg_indices]\n",
    "    for pos in pos_scores:\n",
    "        margin = 1.0\n",
    "        loss += torch.sum(F.relu(margin - pos + neg_scores))\n",
    "    return loss / len(pos_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9363081f-a50d-4f6d-a42c-f7e241518ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MRR & F1 Evaluation ===\n",
    "def compute_mrr(scores, positive_ids):\n",
    "    sorted_indices = torch.argsort(scores, descending=True)\n",
    "    ranks = [(sorted_indices == pid).nonzero(as_tuple=True)[0].item() + 1 for pid in positive_ids if pid < len(scores)]\n",
    "    reciprocals = [1.0 / rank for rank in ranks]\n",
    "    return sum(reciprocals) / len(ranks) if ranks else 0.0\n",
    "\n",
    "def compute_f1_at_k(scores, positive_ids, k=5):\n",
    "    top_k = torch.argsort(scores, descending=True)[:k]\n",
    "    retrieved_set = set(top_k.tolist())\n",
    "    gold_set = set(positive_ids.tolist())\n",
    "    tp = len(retrieved_set & gold_set)\n",
    "    precision = tp / k\n",
    "    recall = tp / len(gold_set) if gold_set else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26a83edc-e05f-4f70-971d-5d8145290ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6365251a-e8e5-4fee-bc54-0d0d30a29134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "# === Load Data ===\n",
    "train_graphs = torch.load('data/train.pt')\n",
    "train_list = [v['graph'] for v in train_graphs.values()]\n",
    "train_loader = DataLoader(train_list, batch_size=1, shuffle=True)\n",
    "\n",
    "# === Initialize Model and Optimizer ===\n",
    "model = GNNRanker(input_dim=768, hidden_dim=256).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0de8677-4a5c-489b-8d39-e0453dbbbca0",
   "metadata": {},
   "source": [
    "# Mean Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a00d83b-da98-4dcb-8ecc-5c3977780184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████████| 167454/167454 [29:10<00:00, 95.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.3840 | MRR: 0.6766 | F1@5: 0.6313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████████████████████████████████████████████████████████████| 167454/167454 [26:38<00:00, 104.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss: 0.2549 | MRR: 0.6856 | F1@5: 0.6333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|███████████████████████████████████████████████████████████████████| 167454/167454 [29:11<00:00, 95.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss: 0.2282 | MRR: 0.6874 | F1@5: 0.6339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████████████████████████████████████████████████████████████| 167454/167454 [26:10<00:00, 106.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss: 0.2139 | MRR: 0.6885 | F1@5: 0.6341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|███████████████████████████████████████████████████████████████████| 167454/167454 [28:09<00:00, 99.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss: 0.2024 | MRR: 0.6892 | F1@5: 0.6343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|███████████████████████████████████████████████████████████████████| 167454/167454 [47:39<00:00, 58.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Loss: 0.1832 | MRR: 0.6907 | F1@5: 0.6347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|█████████████████████████████████████████████████████████████████| 167454/167454 [1:48:16<00:00, 25.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 | Loss: 0.1801 | MRR: 0.6911 | F1@5: 0.6348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10:   5%|███▎                                                                | 8196/167454 [01:26<27:40, 95.89it/s]"
     ]
    }
   ],
   "source": [
    "# === Training Loop ===\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_mrr = 0\n",
    "    total_f1 = 0\n",
    "    batches = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        batch = batch.to(DEVICE)\n",
    "\n",
    "        if batch.edges_index is None:\n",
    "            continue\n",
    "        if batch.edges_index.dtype != torch.long:\n",
    "            batch.edges_index = batch.edges_index.long()\n",
    "\n",
    "        scores = model(batch.x, batch.edges_index)\n",
    "        positive_ids = batch.positive_ids.to(DEVICE)\n",
    "\n",
    "        loss = ranking_loss(scores, positive_ids)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_mrr += compute_mrr(scores, positive_ids)\n",
    "        total_f1 += compute_f1_at_k(scores, positive_ids)\n",
    "        batches += 1\n",
    "\n",
    "    avg_loss = total_loss / batches if batches else 0\n",
    "    avg_mrr = total_mrr / batches if batches else 0\n",
    "    avg_f1 = total_f1 / batches if batches else 0\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | MRR: {avg_mrr:.4f} | F1@5: {avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2e65f7f-6cf7-4264-ab55-3b5ea56a232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_graphs = torch.load('data/dev.pt')\n",
    "dev_list = [v['graph'] for v in dev_graphs.values()]\n",
    "dev_loader = DataLoader(dev_list, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "999e9370-7248-4c85-b573-579c54de0c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 12576/12576 [01:04<00:00, 196.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dev Eval] MRR: 0.6462 | F1@5: 0.6258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_mrr = 0.0\n",
    "total_f1 = 0.0\n",
    "batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dev_loader):\n",
    "        batch = batch.to(DEVICE)\n",
    "\n",
    "        if batch.edges_index.dtype != torch.long:\n",
    "            batch.edges_index = batch.edges_index.long()\n",
    "\n",
    "        scores = model(batch.x, batch.edges_index)\n",
    "        positive_ids = batch.positive_ids.to(DEVICE)\n",
    "\n",
    "        mrr = compute_mrr(scores, positive_ids)\n",
    "        f1 = compute_f1_at_k(scores, positive_ids, k=5)\n",
    "\n",
    "        total_mrr += mrr\n",
    "        total_f1 += f1\n",
    "        batches += 1\n",
    "\n",
    "print(f\"[Dev Eval] MRR: {total_mrr / batches:.4f} | F1@5: {total_f1 / batches:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6f3946-386d-43b7-97c2-677cc2939d26",
   "metadata": {},
   "source": [
    "#Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0ddc5be-e53a-4994-bd2d-67f0ea196f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 12576/12576 [21:31<00:00,  9.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average F1: 0.2339\n",
      "Average ROUGE-L: 0.2350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load LLM\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-small').to(DEVICE)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "def compute_f1(prediction, ground_truth):\n",
    "    pred_tokens = word_tokenize(prediction.lower())\n",
    "    gold_tokens = word_tokenize(ground_truth.lower())\n",
    "    common = set(pred_tokens) & set(gold_tokens)\n",
    "    if len(common) == 0:\n",
    "        return 0.0\n",
    "    precision = len(common) / len(pred_tokens)\n",
    "    recall = len(common) / len(gold_tokens)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "# === Evaluation Loop ===\n",
    "total_f1, total_rouge = 0.0, 0.0\n",
    "count = 0\n",
    "model.eval()\n",
    "\n",
    "for i, (qid, sample) in enumerate(tqdm(dev_graphs.items())):\n",
    "    graph = sample['graph'].to(DEVICE)\n",
    "    query = sample['query']\n",
    "    passage_titles = sample['passage_titles']\n",
    "    positive_ids = sample['positive_ids']\n",
    "\n",
    "    # Gold answer (concatenated titles of gold passages)\n",
    "    gold_answer = \" \".join([passage_titles[i] for i in positive_ids])\n",
    "\n",
    "    # GNN Scoring + Ranking\n",
    "    with torch.no_grad():\n",
    "        scores = model(graph.x, graph.edges_index)\n",
    "    topk = torch.argsort(scores, descending=True)[:5]\n",
    "    top_passages = [passage_titles[i] for i in topk.cpu()]\n",
    "    context = \" \".join(top_passages)\n",
    "\n",
    "    # LLM Input\n",
    "    prompt = f\"question: {query} context: {context}\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    # Generate Answer\n",
    "    output = t5_model.generate(input_ids, max_length=50)\n",
    "    prediction = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Score\n",
    "    f1 = compute_f1(prediction, gold_answer)\n",
    "    rouge = scorer.score(prediction, gold_answer)['rougeL'].fmeasure\n",
    "\n",
    "    total_f1 += f1\n",
    "    total_rouge += rouge\n",
    "    count += 1\n",
    "\n",
    "# Final Results\n",
    "print(f\"\\nAverage F1: {total_f1 / count:.4f}\")\n",
    "print(f\"Average ROUGE-L: {total_rouge / count:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8fd47a73-af01-4bb1-89ae-a49ea74d68ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7385c73d-1b39-4418-b2fd-6aa52c6ae91b",
   "metadata": {},
   "source": [
    "#Max Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d603a2e-efe3-445f-ad98-f5792f9d9ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch_geometric/warnings.py:11: UserWarning: The usage of `scatter(reduce='max')` can be accelerated via the 'torch-scatter' package, but it was not found\n",
      "  warnings.warn(message)\n",
      "Epoch 1: 100%|██████████████████████████████████████████████████████████████████| 167454/167454 [19:11<00:00, 145.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.2561 | MRR: 0.6873 | F1@5: 0.6334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.██████████████████▏                                  | 80467/167454 [10:38<13:14, 109.45it/s]\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 3: 100%|██████████████████████████████████████████████████████████████████| 167454/167454 [21:09<00:00, 131.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss: 0.1600 | MRR: 0.6953 | F1@5: 0.6355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████████████████████████████████████████████████████████████| 167454/167454 [23:41<00:00, 117.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss: 0.1502 | MRR: 0.6966 | F1@5: 0.6359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.█████████████████████████████▋                      | 110864/167454 [16:56<08:20, 113.08it/s]\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Training Loop ===\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_mrr = 0\n",
    "    total_f1 = 0\n",
    "    batches = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        batch = batch.to(DEVICE)\n",
    "\n",
    "        if batch.edges_index is None:\n",
    "            continue\n",
    "        if batch.edges_index.dtype != torch.long:\n",
    "            batch.edges_index = batch.edges_index.long()\n",
    "\n",
    "        scores = model(batch.x, batch.edges_index)\n",
    "        positive_ids = batch.positive_ids.to(DEVICE)\n",
    "\n",
    "        loss = ranking_loss(scores, positive_ids)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_mrr += compute_mrr(scores, positive_ids)\n",
    "        total_f1 += compute_f1_at_k(scores, positive_ids)\n",
    "        batches += 1\n",
    "\n",
    "    avg_loss = total_loss / batches if batches else 0\n",
    "    avg_mrr = total_mrr / batches if batches else 0\n",
    "    avg_f1 = total_f1 / batches if batches else 0\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | MRR: {avg_mrr:.4f} | F1@5: {avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f58f478-533d-4929-9c0f-94e8dc2202d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 12576/12576 [00:55<00:00, 226.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dev Eval] MRR: 0.6658 | F1@5: 0.6319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_mrr = 0.0\n",
    "total_f1 = 0.0\n",
    "batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dev_loader):\n",
    "        batch = batch.to(DEVICE)\n",
    "\n",
    "        if batch.edges_index.dtype != torch.long:\n",
    "            batch.edges_index = batch.edges_index.long()\n",
    "\n",
    "        scores = model(batch.x, batch.edges_index)\n",
    "        positive_ids = batch.positive_ids.to(DEVICE)\n",
    "\n",
    "        mrr = compute_mrr(scores, positive_ids)\n",
    "        f1 = compute_f1_at_k(scores, positive_ids, k=5)\n",
    "\n",
    "        total_mrr += mrr\n",
    "        total_f1 += f1\n",
    "        batches += 1\n",
    "\n",
    "print(f\"[Dev Eval] MRR: {total_mrr / batches:.4f} | F1@5: {total_f1 / batches:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0329f233-2779-4e15-83a4-ef471bc05fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27a8f626-d750-4485-a767-83ad2706aa70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 12576/12576 [15:56<00:00, 13.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average F1: 0.2205\n",
      "Average ROUGE-L: 0.2218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load LLM\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-small').to(DEVICE)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "def compute_f1(prediction, ground_truth):\n",
    "    pred_tokens = word_tokenize(prediction.lower())\n",
    "    gold_tokens = word_tokenize(ground_truth.lower())\n",
    "    common = set(pred_tokens) & set(gold_tokens)\n",
    "    if len(common) == 0:\n",
    "        return 0.0\n",
    "    precision = len(common) / len(pred_tokens)\n",
    "    recall = len(common) / len(gold_tokens)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "# === Evaluation Loop ===\n",
    "total_f1, total_rouge = 0.0, 0.0\n",
    "count = 0\n",
    "model.eval()\n",
    "\n",
    "for i, (qid, sample) in enumerate(tqdm(dev_graphs.items())):\n",
    "    graph = sample['graph'].to(DEVICE)\n",
    "    query = sample['query']\n",
    "    passage_titles = sample['passage_titles']\n",
    "    positive_ids = sample['positive_ids']\n",
    "\n",
    "    # Gold answer (concatenated titles of gold passages)\n",
    "    gold_answer = \" \".join([passage_titles[i] for i in positive_ids])\n",
    "\n",
    "    # GNN Scoring + Ranking\n",
    "    with torch.no_grad():\n",
    "        scores = model(graph.x, graph.edges_index)\n",
    "    topk = torch.argsort(scores, descending=True)[:5]\n",
    "    top_passages = [passage_titles[i] for i in topk.cpu()]\n",
    "    context = \" \".join(top_passages)\n",
    "\n",
    "    # LLM Input\n",
    "    prompt = f\"question: {query} context: {context}\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    # Generate Answer\n",
    "    output = t5_model.generate(input_ids, max_length=50)\n",
    "    prediction = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Score\n",
    "    f1 = compute_f1(prediction, gold_answer)\n",
    "    rouge = scorer.score(prediction, gold_answer)['rougeL'].fmeasure\n",
    "\n",
    "    total_f1 += f1\n",
    "    total_rouge += rouge\n",
    "    count += 1\n",
    "\n",
    "# Final Results\n",
    "print(f\"\\nAverage F1: {total_f1 / count:.4f}\")\n",
    "print(f\"Average ROUGE-L: {total_rouge / count:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d457ff8-61db-4eb8-8053-42ada4d97a15",
   "metadata": {},
   "source": [
    "#Sum Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c0eed6c-7892-4734-8395-141433f2b6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████████████████████████████████████| 167454/167454 [25:41<00:00, 108.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.4131 | MRR: 0.6840 | F1@5: 0.6319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.                                                     | 30590/167454 [04:43<17:20, 131.48it/s]\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 2: 100%|██████████████████████████████████████████████████████████████████| 167454/167454 [25:35<00:00, 109.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss: 0.3813 | MRR: 0.6906 | F1@5: 0.6339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.█████████████████████████████                        | 107554/167454 [16:17<10:16, 97.20it/s]\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 3: 100%|██████████████████████████████████████████████████████████████████| 167454/167454 [26:27<00:00, 105.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss: 0.4082 | MRR: 0.6927 | F1@5: 0.6345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.█▉                                                    | 39121/167454 [09:15<39:20, 54.38it/s]\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 4: 100%|█████████████████████████████████████████████████████████████████| 167454/167454 [1:05:38<00:00, 42.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss: 0.4084 | MRR: 0.6941 | F1@5: 0.6348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.███████████████████████▊                              | 93140/167454 [32:30<21:37, 57.27it/s]\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Epoch 5: 100%|███████████████████████████████████████████████████████████████████| 167454/167454 [58:32<00:00, 47.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss: 0.4646 | MRR: 0.6951 | F1@5: 0.6351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Training Loop ===\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_mrr = 0\n",
    "    total_f1 = 0\n",
    "    batches = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        batch = batch.to(DEVICE)\n",
    "\n",
    "        if batch.edges_index is None:\n",
    "            continue\n",
    "        if batch.edges_index.dtype != torch.long:\n",
    "            batch.edges_index = batch.edges_index.long()\n",
    "\n",
    "        scores = model(batch.x, batch.edges_index)\n",
    "        positive_ids = batch.positive_ids.to(DEVICE)\n",
    "\n",
    "        loss = ranking_loss(scores, positive_ids)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_mrr += compute_mrr(scores, positive_ids)\n",
    "        total_f1 += compute_f1_at_k(scores, positive_ids)\n",
    "        batches += 1\n",
    "\n",
    "    avg_loss = total_loss / batches if batches else 0\n",
    "    avg_mrr = total_mrr / batches if batches else 0\n",
    "    avg_f1 = total_f1 / batches if batches else 0\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | MRR: {avg_mrr:.4f} | F1@5: {avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba2aeaf3-9cfa-4ab6-bff9-d88ef1312b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 12576/12576 [01:53<00:00, 110.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dev Eval] MRR: 0.6569 | F1@5: 0.6269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_mrr = 0.0\n",
    "total_f1 = 0.0\n",
    "batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dev_loader):\n",
    "        batch = batch.to(DEVICE)\n",
    "\n",
    "        if batch.edges_index.dtype != torch.long:\n",
    "            batch.edges_index = batch.edges_index.long()\n",
    "\n",
    "        scores = model(batch.x, batch.edges_index)\n",
    "        positive_ids = batch.positive_ids.to(DEVICE)\n",
    "\n",
    "        mrr = compute_mrr(scores, positive_ids)\n",
    "        f1 = compute_f1_at_k(scores, positive_ids, k=5)\n",
    "\n",
    "        total_mrr += mrr\n",
    "        total_f1 += f1\n",
    "        batches += 1\n",
    "\n",
    "print(f\"[Dev Eval] MRR: {total_mrr / batches:.4f} | F1@5: {total_f1 / batches:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f8aad26-7e61-47aa-89de-7485557ccbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63697368-b83c-475d-948d-98d431fcde66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 12576/12576 [26:31<00:00,  7.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average F1: 0.2259\n",
      "Average ROUGE-L: 0.2264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load LLM\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-small').to(DEVICE)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "def compute_f1(prediction, ground_truth):\n",
    "    pred_tokens = word_tokenize(prediction.lower())\n",
    "    gold_tokens = word_tokenize(ground_truth.lower())\n",
    "    common = set(pred_tokens) & set(gold_tokens)\n",
    "    if len(common) == 0:\n",
    "        return 0.0\n",
    "    precision = len(common) / len(pred_tokens)\n",
    "    recall = len(common) / len(gold_tokens)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "# === Evaluation Loop ===\n",
    "total_f1, total_rouge = 0.0, 0.0\n",
    "count = 0\n",
    "model.eval()\n",
    "\n",
    "for i, (qid, sample) in enumerate(tqdm(dev_graphs.items())):\n",
    "    graph = sample['graph'].to(DEVICE)\n",
    "    query = sample['query']\n",
    "    passage_titles = sample['passage_titles']\n",
    "    positive_ids = sample['positive_ids']\n",
    "\n",
    "    # Gold answer (concatenated titles of gold passages)\n",
    "    gold_answer = \" \".join([passage_titles[i] for i in positive_ids])\n",
    "\n",
    "    # GNN Scoring + Ranking\n",
    "    with torch.no_grad():\n",
    "        scores = model(graph.x, graph.edges_index)\n",
    "    topk = torch.argsort(scores, descending=True)[:5]\n",
    "    top_passages = [passage_titles[i] for i in topk.cpu()]\n",
    "    context = \" \".join(top_passages)\n",
    "\n",
    "    # LLM Input\n",
    "    prompt = f\"question: {query} context: {context}\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    # Generate Answer\n",
    "    output = t5_model.generate(input_ids, max_length=50)\n",
    "    prediction = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Score\n",
    "    f1 = compute_f1(prediction, gold_answer)\n",
    "    rouge = scorer.score(prediction, gold_answer)['rougeL'].fmeasure\n",
    "\n",
    "    total_f1 += f1\n",
    "    total_rouge += rouge\n",
    "    count += 1\n",
    "\n",
    "# Final Results\n",
    "print(f\"\\nAverage F1: {total_f1 / count:.4f}\")\n",
    "print(f\"Average ROUGE-L: {total_rouge / count:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54647510-3386-4daa-80a3-de94672eb219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
