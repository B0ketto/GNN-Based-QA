{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7a2c5c4-467d-437c-bdcc-a992ae0fdfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Apr 12 21:17:18 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off | 00000000:17:00.0 Off |                    0 |\n",
      "| N/A   58C    P0             190W / 300W |  23874MiB / 81920MiB |     29%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off | 00000000:31:00.0 Off |                    0 |\n",
      "| N/A   34C    P0              46W / 300W |      7MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          Off | 00000000:4B:00.0 Off |                    0 |\n",
      "| N/A   56C    P0              74W / 300W |  39878MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          Off | 00000000:CA:00.0 Off |                    0 |\n",
      "| N/A   32C    P0              41W / 300W |      9MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c12a041-5fed-4b5f-bb4d-7d4c4283ec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import DataLoader\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe0ba68-d21a-4705-83e5-3733cd9c4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f75340e-1393-4100-abc7-de26fb310a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# === GNN Ranker Model ===\n",
    "class GNNRanker(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(GNNRanker, self).__init__()\n",
    "        \n",
    "        #Mean Aggregation\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.conv4 = GCNConv(hidden_dim, hidden_dim)# third layer\n",
    "        self.scorer = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        #Max Aggrgation\n",
    "        # self.conv1 = SAGEConv(input_dim, hidden_dim, aggr='max')\n",
    "        # self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr='max')\n",
    "\n",
    "        #Min Aggrgation\n",
    "        # self.conv1 = SAGEConv(input_dim, hidden_dim, aggr='min')\n",
    "        # self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr='min')\n",
    "\n",
    "        #Sum Aggrgation\n",
    "        # self.conv1 = SAGEConv(input_dim, hidden_dim, aggr='sum')\n",
    "        # self.conv2 = SAGEConv(hidden_dim, hidden_dim, aggr='sum')\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = self.conv4(x, edge_index)\n",
    "        return self.scorer(x).squeeze(-1)  # shape: [num_nodes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b2cea21-f953-41c2-b548-429008acd870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Loss Function ===\n",
    "def ranking_loss(scores, positive_ids):\n",
    "    loss = 0.0\n",
    "    pos_scores = scores[positive_ids]\n",
    "    all_indices = torch.arange(len(scores), device=scores.device)\n",
    "    neg_indices = list(set(all_indices.tolist()) - set(positive_ids.tolist()))\n",
    "    \n",
    "    if len(neg_indices) == 0 or len(pos_scores) == 0:\n",
    "        return torch.tensor(0.0, device=scores.device, requires_grad=True)\n",
    "\n",
    "    neg_scores = scores[neg_indices]\n",
    "    for pos in pos_scores:\n",
    "        margin = 1.0\n",
    "        loss += torch.sum(F.relu(margin - pos + neg_scores))\n",
    "    return loss / len(pos_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9363081f-a50d-4f6d-a42c-f7e241518ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MRR & F1 Evaluation ===\n",
    "def compute_mrr(scores, positive_ids):\n",
    "    sorted_indices = torch.argsort(scores, descending=True)\n",
    "    ranks = [(sorted_indices == pid).nonzero(as_tuple=True)[0].item() + 1 for pid in positive_ids if pid < len(scores)]\n",
    "    reciprocals = [1.0 / rank for rank in ranks]\n",
    "    return sum(reciprocals) / len(ranks) if ranks else 0.0\n",
    "\n",
    "def compute_f1_at_k(scores, positive_ids, k=5):\n",
    "    top_k = torch.argsort(scores, descending=True)[:k]\n",
    "    retrieved_set = set(top_k.tolist())\n",
    "    gold_set = set(positive_ids.tolist())\n",
    "    tp = len(retrieved_set & gold_set)\n",
    "    precision = tp / k\n",
    "    recall = tp / len(gold_set) if gold_set else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26a83edc-e05f-4f70-971d-5d8145290ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6365251a-e8e5-4fee-bc54-0d0d30a29134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "# === Load Data ===\n",
    "train_graphs = torch.load('data/train.pt')\n",
    "train_list = [v['graph'] for v in train_graphs.values()]\n",
    "train_loader = DataLoader(train_list, batch_size=1, shuffle=True)\n",
    "\n",
    "# === Initialize Model and Optimizer ===\n",
    "model = GNNRanker(input_dim=768, hidden_dim=256).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0de8677-4a5c-489b-8d39-e0453dbbbca0",
   "metadata": {},
   "source": [
    "# Mean Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a00d83b-da98-4dcb-8ecc-5c3977780184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████████| 167454/167454 [28:41<00:00, 97.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.2005 | MRR: 0.6892 | F1@5: 0.6339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|███████████████████████████████████████████████████████████████████| 167454/167454 [28:55<00:00, 96.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | Loss: 0.1888 | MRR: 0.6900 | F1@5: 0.6341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|███████████████████████████████████████████████████████████████████| 167454/167454 [28:18<00:00, 98.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | Loss: 0.1806 | MRR: 0.6904 | F1@5: 0.6342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████████████████████████████████████████████████████████████| 167454/167454 [25:38<00:00, 108.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 | Loss: 0.1736 | MRR: 0.6909 | F1@5: 0.6342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████████████████████████████████████████████████████████████| 167454/167454 [25:36<00:00, 108.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 | Loss: 0.1693 | MRR: 0.6912 | F1@5: 0.6343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === Training Loop ===\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_mrr = 0\n",
    "    total_f1 = 0\n",
    "    batches = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        batch = batch.to(DEVICE)\n",
    "\n",
    "        if batch.edges_index is None:\n",
    "            continue\n",
    "        if batch.edges_index.dtype != torch.long:\n",
    "            batch.edges_index = batch.edges_index.long()\n",
    "\n",
    "        scores = model(batch.x, batch.edges_index)\n",
    "        positive_ids = batch.positive_ids.to(DEVICE)\n",
    "\n",
    "        loss = ranking_loss(scores, positive_ids)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_mrr += compute_mrr(scores, positive_ids)\n",
    "        total_f1 += compute_f1_at_k(scores, positive_ids)\n",
    "        batches += 1\n",
    "\n",
    "    avg_loss = total_loss / batches if batches else 0\n",
    "    avg_mrr = total_mrr / batches if batches else 0\n",
    "    avg_f1 = total_f1 / batches if batches else 0\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} | MRR: {avg_mrr:.4f} | F1@5: {avg_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610ef9c9-ad9f-4225-bc08-c55634e2c1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6922c1-6fa3-4724-a02e-522a4d130ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list['edge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2e65f7f-6cf7-4264-ab55-3b5ea56a232f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_graphs = torch.load('data/dev.pt')\n",
    "dev_list = [v['graph'] for v in dev_graphs.values()]\n",
    "dev_loader = DataLoader(dev_list, batch_size=1, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "999e9370-7248-4c85-b573-579c54de0c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 12576/12576 [00:34<00:00, 364.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dev Eval] MRR: 0.6518 | F1@5: 0.6251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_mrr = 0.0\n",
    "total_f1 = 0.0\n",
    "batches = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dev_loader):\n",
    "        batch = batch.to(DEVICE)\n",
    "\n",
    "        if batch.edges_index.dtype != torch.long:\n",
    "            batch.edges_index = batch.edges_index.long()\n",
    "\n",
    "        scores = model(batch.x, batch.edges_index)\n",
    "        positive_ids = batch.positive_ids.to(DEVICE)\n",
    "\n",
    "        mrr = compute_mrr(scores, positive_ids)\n",
    "        f1 = compute_f1_at_k(scores, positive_ids, k=5)\n",
    "\n",
    "        total_mrr += mrr\n",
    "        total_f1 += f1\n",
    "        batches += 1\n",
    "\n",
    "print(f\"[Dev Eval] MRR: {total_mrr / batches:.4f} | F1@5: {total_f1 / batches:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92eb22f3-66dc-42b6-97c4-bc2573c8cab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91364747-0b3e-4584-9098-b83870b418be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 12576/12576 [19:26<00:00, 10.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average F1: 0.2347\n",
      "Average ROUGE-L: 0.2353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk import word_tokenize\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load LLM\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained('t5-small').to(DEVICE)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "def compute_f1(prediction, ground_truth):\n",
    "    pred_tokens = word_tokenize(prediction.lower())\n",
    "    gold_tokens = word_tokenize(ground_truth.lower())\n",
    "    common = set(pred_tokens) & set(gold_tokens)\n",
    "    if len(common) == 0:\n",
    "        return 0.0\n",
    "    precision = len(common) / len(pred_tokens)\n",
    "    recall = len(common) / len(gold_tokens)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "# === Evaluation Loop ===\n",
    "total_f1, total_rouge = 0.0, 0.0\n",
    "count = 0\n",
    "model.eval()\n",
    "\n",
    "for i, (qid, sample) in enumerate(tqdm(dev_graphs.items())):\n",
    "    graph = sample['graph'].to(DEVICE)\n",
    "    query = sample['query']\n",
    "    passage_titles = sample['passage_titles']\n",
    "    positive_ids = sample['positive_ids']\n",
    "\n",
    "    # Gold answer (concatenated titles of gold passages)\n",
    "    gold_answer = \" \".join([passage_titles[i] for i in positive_ids])\n",
    "\n",
    "    # GNN Scoring + Ranking\n",
    "    with torch.no_grad():\n",
    "        scores = model(graph.x, graph.edges_index)\n",
    "    topk = torch.argsort(scores, descending=True)[:5]\n",
    "    top_passages = [passage_titles[i] for i in topk.cpu()]\n",
    "    context = \" \".join(top_passages)\n",
    "\n",
    "    # LLM Input\n",
    "    prompt = f\"question: {query} context: {context}\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    # Generate Answer\n",
    "    output = t5_model.generate(input_ids, max_length=50)\n",
    "    prediction = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    # Score\n",
    "    f1 = compute_f1(prediction, gold_answer)\n",
    "    rouge = scorer.score(prediction, gold_answer)['rougeL'].fmeasure\n",
    "\n",
    "    total_f1 += f1\n",
    "    total_rouge += rouge\n",
    "    count += 1\n",
    "\n",
    "# Final Results\n",
    "print(f\"\\nAverage F1: {total_f1 / count:.4f}\")\n",
    "print(f\"Average ROUGE-L: {total_rouge / count:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a2a739-e131-4101-aed8-4f14676ab51d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
